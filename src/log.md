[<img width=900 src="https://raw.githubusercontent.com/txt/mase/master/img/banner1.png">](https://github.com/txt/mase/blob/master/README.md)   
[At a glance...](https://github.com/txt/mase/blob/master/OVERVIEW.md) |
[Syllabus](https://github.com/txt/mase/blob/master/SYLLABUS.md) |
[Models](https://github.com/txt/mase/blob/master/MODELS.md) |
[Code](https://github.com/txt/mase/tree/master/src) |
[Lecturer](http://menzies.us) 



Logs are places to store records of past events. There are two types of logs:

+ _Num_ : for numbers
+ _Sym_ : for everything else. 

Those logs can be queried to find e.g. the highest
and lowest value of the number seen so far. Alternatively,
they can be queried to return values at the same probability
as the current log contents.

### Max Log Size

To avoid logs consuming all memory, logs store at
most _The.cache.keep_ entries (e.g. 128):

+ If more
than that number of entries arrive, then some old
entry (selected at random) will be deleted.
+ The nature of this cache means that some rare
events might be missed. To check for that, running
the code multiple times and, each time, double the
cache size. Stop when doubling the cache size stops
changing the output.

Just as an example of that process, here we are logging 1,000,000 numbers in a log with a cache of size 16.
Note that the resulting cache is much smaller than 1,000,000 items. Also, the contents of the cache
come from the entire range one to one million (so our log is not biased to just the first few samples:

     % python -i log.py
     >>> The.cache.keep = 16
     >>> log = Num()  
     >>> for x in xrange(1000000): log += x 
     >>> sorted(log._cache)
     [77748, 114712, 122521, 224268, 
     289880, 313675, 502464, 625036, 
     661881, 663207, 680085, 684674, 
     867075, 875594, 922141, 945896]
     >>> 

### Caching Slow Reports

Some of the things we want to report from these logs take a little while to calculate (e.g. finding the median
requires a sort of a numeric cache):

+ Such reports should be run and cached so they can be accessed many time without the need
for tedious recalculation. 
+ These reports become outdated if new log information arrives so the following
code deletes these reports if ever new data arrives.
+ The protocol for access those reports is to call _log.has().x_ where "x" is a field
  generated by the report.  Log subclasses generate reports using the special _report()_ method
  (see examples, below).

Just as an example of reporting, after the above run (where we logged 1,000,000 numbers), the following reports are available:

     >>> log.has().lo
     0 
     >>> log.has().hi
     945896
     >>> print log.has().median # 50th percentile
     662544.0
     >>> print log.has().iqr # (75-25)th percentile
     205194

Note that our median is not as expected (it should be around half a million). Why? Well, clearly a cache of size 16 is
too small to track a million numbers. So how many numbers do we need? Well, that depends on the distribution being explored
but here's how the median is effected by cache size for uniform distributions:

    >>> for size in [16,32,64,128,256]:
    ...     The.cache.keep=size
    ...     log = Num()
    ...     for x in xrange(1000000): log += x
    ...     print size, ":" log.has().median
    ... 
     16 : 637374.5
     32 : 480145.5
     64 : 520585.5
    128 : 490742.0
    256 : 470870.5


Note that we get pretty close to half a million with cache sizes at 32 or above. And the lesson: sometimes, a limited
sample can offer a useful approximation to a seemingly complex process.

## Standard Header
<a href="log.py#L88-L93"><img align=right src="http://www.hungarianreference.com/i/arrow_out.gif"></a><br clear=all>
```python

   1:   from __future__ import division
   2:   import sys, random, math, datetime, time,re
   3:   sys.dont_write_bytecode = True
   4:   from base  import *
   5:   from stats import *
   6:   from a12 import *
```
## Classes

### Base Class: "Log"

<a href="log.py#L100-L128"><img align=right src="http://www.hungarianreference.com/i/arrow_out.gif"></a><br clear=all>
```python

   1:   class Log():
   2:     "Keep a random sample of stuff seen so far."
   3:     def __init__(i,inits=[],label=''):
   4:       i.label = label
   5:       i._cache,i.n,i._report = [],0,None
   6:       i.setup()
   7:       map(i.__iadd__,inits)
   8:     def __iadd__(i,x): #  magic method for "+="
   9:       if x == None: return x # skip nothing
  10:       i.n += 1
  11:       changed = False
  12:       if len(i._cache) < The.cache.keep: # not full
  13:         changed = True
  14:         i._cache += [x]               # then add
  15:       else: # otherwise, maybe replace an old item
  16:         if rand() <= The.cache.keep/i.n:
  17:           changed = True
  18:           i._cache[int(rand()*The.cache.keep)] = x
  19:       if changed:      
  20:         i._report = None # wipe out 'what follows'
  21:         i.change(x)
  22:       return i
  23:     def any(i):  
  24:       return  any(i._cache)
  25:     def has(i):
  26:       if i._report == None: i._report =  i.report()
  27:       return i._report
  28:     def setup(i): pass
  29:     def change(i,x): pass
```

### Num

A _Num_ is a _Log_ for numbers. 

+ Tracks _lo_ and _hi_ values. 
+ Reports median and the IQR the (75-25)th range.
+ Generates numbers from the log by a three-way interpolation (see _ish()_).


<a href="log.py#L141-L186"><img align=right src="http://www.hungarianreference.com/i/arrow_out.gif"></a><br clear=all>
```python

   1:   class Num(Log):
   2:     def setup(i):
   3:       i.lo, i.hi = 10**32, -10**32
   4:       i.lessp = True
   5:     def change(i,x): # update lo,hi
   6:       i.lo = min(i.lo, x)
   7:       i.hi = max(i.hi, x)
   8:     def norm(i,x): # turn "x" into 0..1
   9:       return (x - i.lo)/(i.hi - i.lo + 0.000001)
  10:     def ordered():
  11:       i.has()
  12:       return i._cache
  13:     def report(i): 
  14:       lst = i._cache = sorted(i._cache)
  15:       n   = len(lst)     
  16:       return o(
  17:         median= i.median(),
  18:         iqr   = lst[int(n*.75)] - lst[int(n*.5)],
  19:         lo    = i.lo, 
  20:         hi    = i.hi)
  21:     def ish(i,f=0.1): # return a num from  logged dist 
  22:       return i.any() + f*(i.any() - i.any())
  23:     def better(new,old):
  24:       "better if (1)less median or (2)same and less iqr"
  25:       t = The.misc.a12
  26:       betterIqr = new.has().iqr < old.has().iqr
  27:       if new.lessp:
  28:         betterMed = new.has().median >= old.has().median
  29:         same      = a12(old._cache, new._cache)  <= t
  30:       else:
  31:         betterMed = new.has().median <= old.has().median 
  32:         same      = a12(new._cache, old._cache) <= t
  33:       return betterMed, same, betterIqr
  34:     def median(i):
  35:       n = len(i._cache)
  36:       p = n // 2
  37:       if (n % 2):  return i._cache[p]
  38:       q = p + 1
  39:       q = max(0,(min(q,n)))
  40:       return (i._cache[p] + i._cache[q])/2
  41:   
  42:   def _num():
  43:     i = Num([rand()      for _ in xrange(1000)])
  44:     j = Num([rand()*1.25 for _ in xrange(1000)])
  45:     print j.same(i)
  46:   
```

WARNING: the call to _sorted_ in _report()_ makes this code
a candidate for a massive CPU suck (it is always sorting newly arrived data).
So distinguish between _adding_ things to a log in the _last_ era and 
using that information in the _next_ era (so the log from the last era
is staple in the current).

### Sym

A _Sym_ is a _Log_ for non-numerics.

+ Tracks frequency counts for symbols, and the most common symbol (the _mode_);
+ Reports the entropy of the space (a measure of diversity: lower values mean fewer rarer symbols);
+ Generated symbols from the log by returning symbols at the same probability of the frequency counts (see _ish()_).

<a href="log.py#L204-L231"><img align=right src="http://www.hungarianreference.com/i/arrow_out.gif"></a><br clear=all>
```python

   1:   class Sym(Log):
   2:     def setup(i):
   3:       i.counts,i.mode,i.most={},None,0
   4:     def report(i):
   5:       for x in i._cache:
   6:         c = i.counts[x] = i.counts.get(x,0) + 1
   7:         if c > i.most:
   8:           i.mode,i.most = x,c
   9:       return o(dist= i.dist(), 
  10:                 ent = i.entropy(),
  11:                 mode= i.mode)
  12:     def dist(i):
  13:       d = i.counts
  14:       n = sum(d.values())
  15:       return sorted([(d[k]/n, k) for k in d.keys()], 
  16:                     reverse=True)
  17:     def ish(i):
  18:       r,tmp = rand(),0
  19:       for w,x in i.has().dist:
  20:         tmp  += w
  21:         if tmp >= r: 
  22:           return x
  23:       return x
  24:     def entropy(i,e=0):
  25:       for k in i.counts:
  26:         p = i.counts[k]/len(i._cache)
  27:         e -= p*log2(p) if p else 0
  28:       return e    
```

#### Sym, Example

As an example of generating numbers from a distribution, consider the following code.
The logged population has plus, grapes and pears in the ration 2:1:1.
From that population, we can generate another distribution that is nearly the same:

    >>> symDemo()
    (0.5, 'plums'), (0.265625, 'grapes'), (0.234375, 'pears')]
    {'plums': 64, 'grapes': 34, 'pears': 30}

<a href="log.py#L245-L254"><img align=right src="http://www.hungarianreference.com/i/arrow_out.gif"></a><br clear=all>
```python

   1:   def symDemo(n1=10,n2=1000):
   2:     rseed()
   3:     log= Sym((['plums']*(n1*2)) + ['grapes']*n1 + ['pears']*n1)
   4:     found= Sym([log.ish() for _ in xrange(n2)])
   5:     print found.has().dist
   6:     print found.counts
   7:     print sum(found.counts.values())
   8:   
   9:   if __name__ == "__main__": eval(cmd()) 
  10:   
  11:   
```


_________

<img align=right src="https://raw.githubusercontent.com/txt/mase/master/img/pd-icon.png">Copyright Â© 2015 [Tim Menzies](http://menzies.us).
This is free and unencumbered software released into the public domain.   
For more details, see the [license](https://github.com/txt/mase/blob/master/LICENSE.md).

